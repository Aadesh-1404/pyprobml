{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autodiff-pytorch.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/book1/supplements/autodiff_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b520E1nCIBHc"
      },
      "source": [
        "\n",
        "# Automatic differentation using PyTorch\n",
        "\n",
        "We show how to do Automatic differentation using PyTorch. We use the NLL for binary logistic regression as the objective.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeuOgABaIENZ"
      },
      "source": [
        "import sklearn\n",
        "import scipy\n",
        "import scipy.optimize\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import itertools\n",
        "import time\n",
        "from functools import partial\n",
        "\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from scipy.special import logsumexp\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPozRwDAKFb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "224e04a5-0d19-4dd1-bd2c-304df0485639"
      },
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "print(\"torch version {}\".format(torch.__version__))\n",
        "if torch.cuda.is_available():\n",
        "  print(torch.cuda.get_device_name(0))\n",
        "  print(\"current device {}\".format(torch.cuda.current_device()))\n",
        "else:\n",
        "  print(\"Torch cannot find GPU\")\n",
        "\n",
        "def set_seed(seed):\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "#torch.backends.cudnn.benchmark = True"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch version 1.8.0+cu101\n",
            "Tesla V100-SXM2-16GB\n",
            "current device 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSYkjaAO6n3A",
        "outputId": "5a6caeb3-42e2-42f3-fd66-d18ca656ae1c"
      },
      "source": [
        "# Fit the model usign sklearn\n",
        "\n",
        "import sklearn.datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = sklearn.datasets.load_iris()\n",
        "X = iris[\"data\"]\n",
        "y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris-Virginica, else 0'\n",
        "N, D = X.shape # 150, 4\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# We set C to a large number to turn off regularization.\n",
        "# We don't fit the bias term to simplify the comparison below.\n",
        "log_reg = LogisticRegression(solver=\"lbfgs\", C=1e5, fit_intercept=False)\n",
        "log_reg.fit(X_train, y_train)\n",
        "w_mle_sklearn = np.ravel(log_reg.coef_)\n",
        "print(w_mle_sklearn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-4.414 -9.111  6.539 12.686]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p5y7b8NbyZp"
      },
      "source": [
        "## Computing gradients by hand\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS5AB9NjLZ_i"
      },
      "source": [
        "\n",
        "\n",
        "# Binary cross entropy\n",
        "def BCE_with_logits(logits, targets):\n",
        "    N = logits.shape[0]\n",
        "    logits = logits.reshape(N,1)\n",
        "    logits_plus = np.hstack([np.zeros((N,1)), logits]) # e^0=1\n",
        "    logits_minus = np.hstack([np.zeros((N,1)), -logits])\n",
        "    logp1 = -logsumexp(logits_minus, axis=1)\n",
        "    logp0 = -logsumexp(logits_plus, axis=1)\n",
        "    logprobs = logp1 * targets + logp0 * (1-targets)\n",
        "    return -np.sum(logprobs)/N\n",
        "\n",
        "# Compute using numpy\n",
        "def sigmoid(x): return 0.5 * (np.tanh(x / 2.) + 1)\n",
        "\n",
        "def predict_logit(weights, inputs):\n",
        "    return np.dot(inputs, weights) # Already vectorized\n",
        "\n",
        "def predict_np(weights, inputs):\n",
        "    return sigmoid(predict_logit(weights, inputs))\n",
        "\n",
        "def NLL(weights, batch):\n",
        "    X, y = batch\n",
        "    logits = predict_logit(weights, X)\n",
        "    return BCE_with_logits(logits, y)\n",
        "\n",
        "def NLL_grad(weights, batch):\n",
        "    X, y = batch\n",
        "    N = X.shape[0]\n",
        "    mu = predict_np(weights, X)\n",
        "    g = np.sum(np.dot(np.diag(mu - y), X), axis=0)/N\n",
        "    return g\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9mD8S18746_",
        "outputId": "e023b766-2aaf-47bd-f552-3575c226e998"
      },
      "source": [
        "w_np = w_mle_sklearn\n",
        "y_pred = predict_np(w_np, X_test)\n",
        "loss_np = NLL(w_np, (X_test, y_test))\n",
        "grad_np = NLL_grad(w_np, (X_test, y_test))\n",
        "print(\"params {}\".format(w_np))\n",
        "#print(\"pred {}\".format(y_pred))\n",
        "print(\"loss {}\".format(loss_np))\n",
        "print(\"grad {}\".format(grad_np))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "params [-4.414 -9.111 6.539 12.686]\n",
            "loss 0.1182400709961879\n",
            "grad [-0.235 -0.122 -0.198 -0.064]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeGQ7SJTNHMk"
      },
      "source": [
        "## PyTorch code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is7yJlgsL4BT"
      },
      "source": [
        "To compute the gradient using torch, we proceed as follows.\n",
        "\n",
        "- declare all the variables that you want to take derivatives with respect to using the requires_grad=True argumnet\n",
        "- define the (scalar output) objective function you want to differentiate in terms of these variables, and evaluate it at a point. This will generate a computation graph and store all the tensors.\n",
        "- call objective.backward() to trigger backpropagation (chain rule) on this graph.\n",
        "- extract the gradients from each variable using variable.grad field. (These will be torch tensors.)\n",
        "\n",
        "See the example below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl_SK0WUlvNl"
      },
      "source": [
        "\n",
        "# data. By default, numpy uses double but torch uses float\n",
        "X_train_t = torch.tensor(X_train,  dtype=torch.float)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.float)\n",
        "\n",
        "X_test_t = torch.tensor(X_test, dtype=torch.float)\n",
        "y_test_t = torch.tensor(y_test, dtype=torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L5NxIaVLu64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4cd1bbd-7069-4e5f-ade7-5e563a0fe11d"
      },
      "source": [
        "# parameters\n",
        "W = np.reshape(w_mle_sklearn, [D, 1]) # convert 1d vector to 2d matrix\n",
        "w_torch = torch.tensor(W, requires_grad=True, dtype=torch.float)\n",
        "#w_torch.requires_grad_() \n",
        "\n",
        "\n",
        "# binary logistic regression in one line of Pytorch\n",
        "def predict(X, w):\n",
        "  y_pred = torch.sigmoid(torch.matmul(X, w))[:,0]\n",
        "  return y_pred\n",
        "\n",
        "# This returns Nx1 probabilities\n",
        "y_pred = predict(X_test_t, w_torch)\n",
        "\n",
        "# loss function is average NLL\n",
        "criterion = torch.nn.BCELoss(reduction='mean')\n",
        "loss_torch = criterion(y_pred, y_test_t)\n",
        "print(loss_torch)\n",
        "\n",
        "# Backprop\n",
        "loss_torch.backward()\n",
        "print(w_torch.grad)\n",
        "\n",
        "# convert to numpy. We have to \"detach\" the gradient tracing feature\n",
        "loss_torch = loss_torch.detach().numpy()\n",
        "grad_torch = w_torch.grad[:,0].detach().numpy()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.1182, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "tensor([[-0.2353],\n",
            "        [-0.1223],\n",
            "        [-0.1976],\n",
            "        [-0.0638]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSKAJvrBNKQC",
        "outputId": "db315c9e-db41-46be-9bea-62f1d6c670c5"
      },
      "source": [
        "# Test\n",
        "assert np.allclose(loss_np, loss_torch)\n",
        "assert np.allclose(grad_np, grad_torch)\n",
        "\n",
        "print(\"loss {}\".format(loss_torch))\n",
        "print(\"grad {}\".format(grad_torch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 0.11824005842208862\n",
            "grad [-0.235 -0.122 -0.198 -0.064]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}