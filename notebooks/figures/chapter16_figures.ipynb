{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "# Use of this source code is governed by an MIT-style\n",
    "# license that can be found in the LICENSE file or at\n",
    "# https://opensource.org/licenses/MIT.\n",
    "\n",
    "# Author(s): Kevin P. Murphy (murphyk@gmail.com) and Mahmoud Soliman (mjs@aucegypt.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://opensource.org/licenses/MIT\" target=\"_parent\"><img src=\"https://img.shields.io/github/license/probml/pyprobml\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/figures//chapter16_figures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloning the pyprobml repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/probml/pyprobml \n",
    "%cd pyprobml/scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing required software (This may take few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install octave  -qq > /dev/null\n",
    "!apt-get install liboctave-dev -qq > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "DISCLAIMER = 'WARNING : Editing in VM - changes lost after reboot!!'\n",
    "from google.colab import files\n",
    "\n",
    "def interactive_script(script, i=True):\n",
    "  if i:\n",
    "    s = open(script).read()\n",
    "    if not s.split('\\n', 1)[0]==\"## \"+DISCLAIMER:\n",
    "      open(script, 'w').write(\n",
    "          f'## {DISCLAIMER}\\n' + '#' * (len(DISCLAIMER) + 3) + '\\n\\n' + s)\n",
    "    files.view(script)\n",
    "    %run $script\n",
    "  else:\n",
    "      %run $script\n",
    "\n",
    "def show_image(img_path):\n",
    "  from google.colab.patches import cv2_imshow\n",
    "  import cv2\n",
    "  img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "  img=cv2.resize(img,(600,600))\n",
    "  cv2_imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 16.1:<a name='16.1'></a> <a name='knn'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  (a) Illustration of a $K$-nearest neighbors classifier in 2d for $K=5$. The nearest neighbors of test point $\\mathbf  x $ have labels $\\ 1, 1, 1, 0, 0\\ $, so we predict $p(y=1|\\mathbf  x , \\mathcal  D  ) = 3/5$. (b) Illustration of the Voronoi tesselation induced by 1-NN. Adapted from Figure 4.13 of <a href='#Duda01'>[DHS01]</a> .  \n",
    "Figure(s) generated by [knn_voronoi_plot.py](https://github.com/probml/pyprobml/blob/master/scripts/knn_voronoi_plot.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"knn_voronoi_plot.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 16.2:<a name='16.2'></a> <a name='knnThreeClass'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Decision boundaries induced by a KNN classifier. (a) $K=1$. (b) $K=2$. (c) $K=5$. (d) Train and test error vs $K$.  \n",
    "Figure(s) generated by [knn_classify_demo.py](https://github.com/probml/pyprobml/blob/master/scripts/knn_classify_demo.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"knn_classify_demo.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 16.3:<a name='16.3'></a> <a name='curse'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Illustration of the curse of dimensionality. (a) We embed a small cube of side $s$ inside a larger unit cube. (b) We plot the edge length of a cube needed to cover a given volume of the unit cube as a function of the number of dimensions. Adapted from Figure 2.6 from <a href='#HastieBook'>[HTF09]</a> .  \n",
    "Figure(s) generated by [curse_dimensionality.py](https://github.com/probml/pyprobml/blob/master/scripts/curse_dimensionality.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"curse_dimensionality.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 16.4:<a name='16.4'></a> <a name='LCA'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Illustration of latent coincidence analysis (LCA) as a directed graphical model. The inputs $\\mathbf  x , \\mathbf  x ' \\in \\mathbb  R ^D$ are mapped into Gaussian latent variables $\\mathbf  z , \\mathbf  z ' \\in \\mathbb  R ^L$ via a linear mapping $\\mathbf  W $. If the two latent points coincide (within length scale $\\kappa $) then we set the similarity label to $y=1$, otherwise we set it to $y=0$. From Figure 1 of <a href='#Der2012'>[ML12]</a> . Used with kind permission of Lawrence Saul. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"/content/pyprobml/notebooks/figures/images/LCA-PGM.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 16.5:<a name='16.5'></a> <a name='tripletNet'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Networks for deep metric learning. (a) Siamese network. (b) Triplet network. From Figure 5 of <a href='#Kaya2019'>[MH19]</a> . Used with kind permission of Mahmut Kaya. . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"/content/pyprobml/notebooks/figures/images/siameseNet.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"/content/pyprobml/notebooks/figures/images/tripletNet.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 16.6:<a name='16.6'></a> <a name='tripletBound'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Speeding up triplet loss minimization. (a) Illustration of hard vs easy negatives. Here $a$ is the anchor point, $p$ is a positive point, and $n_i$ are negative points. Adapted from Figure 4 of <a href='#Kaya2019'>[MH19]</a> . (b) Standard triplet loss would take $8 \\times 3 \\times 4 = 96$ calculations, whereas using a proxy loss (with one proxy per class) takes $8 \\times 2 = 16$ calculations. From Figure 1 of <a href='#Do2019cvpr'>[Tha+19]</a> . Used with kind permission of Gustavo Cerneiro. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"/content/pyprobml/notebooks/figures/images/hard-negative-mining.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"/content/pyprobml/notebooks/figures/images/tripletBound.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 16.7:<a name='16.7'></a> <a name='SEC'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Adding spherical embedding constraint to a deep metric learning method. Used with kind permission of Dingyi Zhang. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"/content/pyprobml/notebooks/figures/images/SEC.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 16.8:<a name='16.8'></a> <a name='smoothingKernels'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  A comparison of some popular normalized kernels.  \n",
    "Figure(s) generated by [smoothingKernelPlot.m](https://github.com/probml/pmtk3/blob/master/demos/smoothingKernelPlot.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W smoothingKernelPlot.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 16.9:<a name='16.9'></a> <a name='parzen'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  A nonparametric (Parzen) density estimator in 1d estimated from 6 data points, denoted by x. Top row: uniform kernel. Bottom row: Gaussian kernel. Left column: bandwidth parameter $h=1$. Right column: bandwidth parameter $h=2$. Adapted from   http://en.wikipedia.org/wiki/Kernel_density_estimation .  \n",
    "Figure(s) generated by [Kernel_density_estimation](http://en.wikipedia.org/wiki/Kernel_density_estimation) [parzen_window_demo2.py](https://github.com/probml/pyprobml/blob/master/scripts/parzen_window_demo2.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"parzen_window_demo2.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 16.10:<a name='16.10'></a> <a name='kernelRegression'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  An example of kernel regression in 1d using a Gaussian kernel.  \n",
    "Figure(s) generated by [kernelRegressionDemo.m](https://github.com/probml/pmtk3/blob/master/demos/kernelRegressionDemo.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W kernelRegressionDemo.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    " <a name='Duda01'>[DHS01]</a> R. O. Duda, P. E. Hart and D. G. Stork. \"Pattern Classification\". (2001). \n",
    "\n",
    "<a name='HastieBook'>[HTF09]</a> T. Hastie, R. Tibshirani and J. Friedman. \"The Elements of Statistical Learning\". (2009). \n",
    "\n",
    "<a name='Kaya2019'>[MH19]</a> K. Mahmut and B. HasanSakir. \"Deep Metric Learning: A Survey\". In: Symmetry (2019). \n",
    "\n",
    "<a name='Der2012'>[ML12]</a> D. Matthew and S. LawrenceK. \"Latent Coincidence Analysis: A Hidden Variable Model forDistance Metric Learning\". (2012). \n",
    "\n",
    "<a name='Do2019cvpr'>[Tha+19]</a> D. Thanh-Toan, T. Toan, R. Ian, K. Vijay, H. Tuan and C. Gustavo. \"A Theoretically Sound Upper Bound on the Triplet Loss forImproving the Efficiency of Deep Distance Metric Learning\". (2019). \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
