{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "# Use of this source code is governed by an MIT-style\n",
    "# license that can be found in the LICENSE file or at\n",
    "# https://opensource.org/licenses/MIT.\n",
    "\n",
    "# Author(s): Kevin P. Murphy (murphyk@gmail.com) and Mahmoud Soliman (mjs@aucegypt.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://opensource.org/licenses/MIT\" target=\"_parent\"><img src=\"https://img.shields.io/github/license/probml/pyprobml\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/figures//AppendixE_figures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloning the pyprobml repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/probml/pyprobml \n",
    "%cd pyprobml/scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "DISCLAIMER = 'WARNING : Editing in VM - changes lost after reboot!!'\n",
    "from google.colab import files\n",
    "\n",
    "def show_and_run(script, i=True):\n",
    "  if i:\n",
    "    s = open(script).read()\n",
    "    if not s.split('\\n', 1)[0]==\"## \"+DISCLAIMER:\n",
    "      open(script, 'w').write(\n",
    "          f'## {DISCLAIMER}\\n' + '#' * (len(DISCLAIMER) + 3) + '\\n\\n' + s)\n",
    "    files.view(script)\n",
    "    %run $script\n",
    "  else:\n",
    "      %run $script\n",
    "\n",
    "def show_image(img_path):\n",
    "  from google.colab.patches import cv2_imshow\n",
    "  import cv2\n",
    "  img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "  img=cv2.resize(img,(600,600))\n",
    "  cv2_imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure E.1:<a name='E.1'></a> <a name='bootstrapDemoBer'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Bootstrap (top row) vs Bayes (bottom row). The $N$ data cases were generated from $\\mathrm  Ber (\\theta =0.7)$. Left column: $N=10$. Right column: $N=100$. (a-b) A bootstrap approximation to the sampling distribution of the MLE for a Bernoulli distribution. We show the histogram derived from $B=10,000$ bootstrap samples. (c-d) Histogram of 10,000 samples from the posterior distribution using a uniform prior.  \n",
    "Figure(s) generated by [bootstrapDemoBer.m](https://github.com/probml/pmtk3/blob/master/demos/bootstrapDemoBer.m) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure E.2:<a name='E.2'></a> <a name='samplingDistGaussShrinkage'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Left: Sampling distribution of the MAP estimate (equivalent to the posterior mean) under a $\\mathcal  N (\\theta _0=0,\\sigma ^2/\\kappa _0)$ prior with different prior strengths $\\kappa _0$. (If we set $\\kappa =0$, the MAP estimate reduces to the MLE.) The data is $n=5$ samples drawn from $\\mathcal  N (\\theta ^*=1,\\sigma ^2=1)$. Right: MSE relative to that of the MLE versus sample size. Adapted from Figure 5.6 of <a href='#Hoff09'>[Hof09]</a> .  \n",
    "Figure(s) generated by [samplingDistGaussShrinkage.m](https://github.com/probml/pmtk3/blob/master/demos/samplingDistGaussShrinkage.m) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure E.3:<a name='E.3'></a> <a name='biasVarianceLinReg'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Illustration of bias-variance tradeoff for ridge regression. We generate 100 data sets from the true function, shown in solid green. Left: we plot the regularized fit for 20 different data sets. We use linear regression with a Gaussian RBF expansion, with 25 centers evenly spread over the $[0,1]$ interval. Right: we plot the average of the fits, averaged over all 100 datasets. Top row: strongly regularized: we see that the individual fits are similar to each other (low variance), but the average is far from the truth (high bias). Bottom row: lightly regularized: we see that the individual fits are quite different from each other (high variance), but the average is close to the truth (low bias). Adapted from <a href='#BishopBook'>[Bis06]</a>  Figure 3.5.  \n",
    "Figure(s) generated by [biasVarModelComplexity3.m](https://github.com/probml/pmtk3/blob/master/demos/biasVarModelComplexity3.m) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure E.4:<a name='E.4'></a> <a name='biasVarianceCartoon'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Cartoon illustration of the bias variance tradeoff. From   http://scott.fortmann-roe.com/docs/BiasVariance.html . Used with kind permission of Scott Fortmann-Roe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"/content/pyprobml/notebooks/figures/images/biasVarCartoon.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure E.5:<a name='E.5'></a> <a name='riskFnGauss'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Risk functions for estimating the mean of a Gaussian. Each curve represents $R( \\theta  _i(\\cdot ),\\theta ^*)$ plotted vs $\\theta ^*$, where $i$ indexes the estimator. Each estimator is applied to $N$ samples from $\\mathcal  N (\\theta ^*,\\sigma ^2=1)$. The dark blue horizontal line is the sample mean (MLE); the red line horizontal line is the sample median; the black curved line is the estimator $ \\theta  =\\theta _0=0$; the green curved line is the posterior mean when $\\kappa =1$; the light blue curved line is the posterior mean when $\\kappa =5$. (a) $N=5$ samples. (b) $N=20$ samples. Adapted from Figure B.1 of <a href='#Bernardo94'>[BS94]</a> .  \n",
    "Figure(s) generated by [riskFnGauss.m](https://github.com/probml/pmtk3/blob/master/demos/riskFnGauss.m) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure E.6:<a name='E.6'></a> <a name='minimaxRisk'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Risk functions for two decision procedures, $\\pi _1$ and $\\pi _2$. Since $\\pi _1$ has lower worst case risk, it is the minimax estimator, even though $\\pi _2$ has lower risk for most values of $\\theta $. Thus minimax estimators are overly conservative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"/content/pyprobml/notebooks/figures/images/{minimaxRiskCurvesCropped}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure E.7:<a name='E.7'></a> <a name='powerCurves'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  (a) Illustration of the Neyman-Pearson hypothesis testing paradigm.  \n",
    "Figure(s) generated by [neymanPearson2.m](https://github.com/probml/pmtk3/blob/master/demos/neymanPearson2.m) [twoPowerCurves.m](https://github.com/probml/pmtk3/blob/master/demos/twoPowerCurves.m) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    " <a name='Bernardo94'>[BS94]</a> J. Bernardo and A. Smith. \"Bayesian Theory\". (1994). \n",
    "\n",
    "<a name='BishopBook'>[Bis06]</a> C. Bishop \"Pattern recognition and machine learning\". (2006). \n",
    "\n",
    "<a name='Hoff09'>[Hof09]</a> P. Hoff \"A First Course in Bayesian Statistical Methods\". (2009). \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
