{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "# Use of this source code is governed by an MIT-style\n",
    "# license that can be found in the LICENSE file or at\n",
    "# https://opensource.org/licenses/MIT.\n",
    "\n",
    "# Author(s): Kevin P. Murphy (murphyk@gmail.com) and Mahmoud Soliman (mjs@aucegypt.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://opensource.org/licenses/MIT\" target=\"_parent\"><img src=\"https://img.shields.io/github/license/probml/pyprobml\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/figures//chapter5_figures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloning the pyprobml repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/probml/pyprobml \n",
    "%cd pyprobml/scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing required software (This may take few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install octave  -qq > /dev/null\n",
    "!apt-get install liboctave-dev -qq > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "DISCLAIMER = 'WARNING : Editing in VM - changes lost after reboot!!'\n",
    "from google.colab import files\n",
    "\n",
    "def interactive_script(script, i=True):\n",
    "  if i:\n",
    "    s = open(script).read()\n",
    "    if not s.split('\\n', 1)[0]==\"## \"+DISCLAIMER:\n",
    "      open(script, 'w').write(\n",
    "          f'## {DISCLAIMER}\\n' + '#' * (len(DISCLAIMER) + 3) + '\\n\\n' + s)\n",
    "    files.view(script)\n",
    "    %run $script\n",
    "  else:\n",
    "      %run $script\n",
    "\n",
    "def show_image(img_path):\n",
    "  from google.colab.patches import cv2_imshow\n",
    "  import cv2\n",
    "  img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "  img=cv2.resize(img,(600,600))\n",
    "  cv2_imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.1:<a name='5.1'></a> <a name='fig:optimaSaddle'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Illustration of a 2d function with a local minimum, global minimum, and a some saddle points. We also illustrate some trajectories through this cost landscape. From Figure 17 from <a href='#Flores-Livas2019'>[Jos+19]</a> . Used with kind permission of Jose Abdenago Flores Livas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"/content/pyprobml/notebooks/figures/images/saddleScreenShot.png.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.2:<a name='5.2'></a> <a name='fig:optimaConstrained'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Illustration of constrained maximization of a nonconvex 1d function. The area between the dotted vertical lines represents the feasible set. (a) There is a unique global maximum since the function is concave within the support of the feasible set. (b) There are two global maxima, both occuring at the boundary of the feasible set. (c) In the unconstrained case, this function has no global maximum, since it is unbounded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"/content/pyprobml/notebooks/figures/images/optima.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.3:<a name='5.3'></a> <a name='fig:nonsmooth'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  (a) Smooth 1d function. (b) Non-smooth 1d function. (There is a discontinuity at the origin.) From   https://scipy-lectures.org/advanced/mathematical_optimization/index.html\\#smooth-and-non-smooth-problems . Used with kind permission of Gael Varoquaux "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"/content/pyprobml/notebooks/figures/images/smooth-fn.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"/content/pyprobml/notebooks/figures/images/nonsmooth-fn.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.4:<a name='5.4'></a> <a name='aokiFixed'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Steepest descent on a simple convex function, starting from $(0,0)$, for 20 steps, using a fixed step size. The global minimum is at $(1,1)$. (a) $\\eta =0.1$. (b) $\\eta =0.6$.  \n",
    "Figure(s) generated by [steepestDescentDemo.m](https://github.com/probml/pmtk3/blob/master/demos/steepestDescentDemo.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W steepestDescentDemo.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.5:<a name='5.5'></a> <a name='fig:steepestDescentKappa'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Illustration of the effect of condition number $\\kappa $ on the convergence speed of steepest descent with exact line searches. (a) Large $\\kappa $. (b) Small $\\kappa $. From Lecture 18 (slides 42, 46) from <a href='#BertsimasOpt'>[Ber09]</a> . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"/content/pyprobml/notebooks/figures/images/steepestDescentLargeKappa.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"/content/pyprobml/notebooks/figures/images/steepestDescentSmallKappa.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.6:<a name='5.6'></a> <a name='aokiLinesearch'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  (a) Steepest descent on the same function as \\cref  fig:aokiFixed , starting from $(0,0)$, using line search.  \n",
    "Figure(s) generated by [steepestDescentDemo.m](https://github.com/probml/pmtk3/blob/master/demos/steepestDescentDemo.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W steepestDescentDemo.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.7:<a name='5.7'></a> <a name='fig:nesterov'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Illustration of the Nesterov update. Adapted from Figure 11.6 of <a href='#Geron2019'>[Aur19]</a> . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"/content/pyprobml/notebooks/figures/images/nesterov-geron-11-6.pdf.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.8:<a name='5.8'></a> <a name='newtonQuad'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Illustration of Newton's method for minimizing a 1d function. (a) The solid curve is the function $\\mathcal  L (x)$. The dotted line $\\mathcal  L _ \\mathrm  quad  (\\theta )$ is its second order approximation at $\\theta _t$. The Newton step $d_t$ is what must be added to $\\theta _t$ to get to the minimum of $\\mathcal  L _ \\mathrm  quad  (\\theta )$. Adapted from Figure 13.4 of <a href='#Vandenberghe06'>[Van06]</a> .  \n",
    "Figure(s) generated by [newtonsMethodMinQuad.m](https://github.com/probml/pmtk3/blob/master/demos/newtonsMethodMinQuad.m) [newtonsMethodNonConvex.m](https://github.com/probml/pmtk3/blob/master/demos/newtonsMethodNonConvex.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W newtonsMethodMinQuad.m >> _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W newtonsMethodNonConvex.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.9:<a name='5.9'></a> <a name='fig:pascanu4-2'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Illustration of the trust region approach. The dashed lines represents contours of the original nonconvex objective. The circles represent successive quadratic approximations. From Figure 4.2 of <a href='#Pascanu2014Thesis'>[Raz14]</a> . Used with kind permission of Razvan Pascanu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"/content/pyprobml/notebooks/figures/images/trustRegion.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.10:<a name='5.10'></a> <a name='fig:honkelaGaussians'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Changing the mean of a Gaussian by a fixed amount (from solid to dotted curve) can have more impact when the (shared) variance is small (as in a) compared to when the variance is large (as in b). Hence the impact (in terms of prediction accuracy) of a change to $\\mu $ depends on where the optimizer is in $(\\mu ,\\sigma )$ space. From Figure 3 of <a href='#Honkela2010'>[Ant+10]</a> , reproduced from <a href='#ValpolaPhD'>[Val00]</a> . Used with kind permission of Antti Honkela. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"/content/pyprobml/notebooks/figures/images/meanchange_lowvar.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"/content/pyprobml/notebooks/figures/images/meanchange_highvar.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.11:<a name='5.11'></a> <a name='fig:NGjascha'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Illustration of the benefits of natural gradient vs steepest descent on a 2D problem. (a) Trajectories of the two methods in parameter space (red = steepest descent, blue = NG). They both start at $(1,-1)$, bottom right location. (b) Objective vs number of iterations. (c) Gradient field in the $\\boldsymbol  \\theta  $ parameter space. (d) Gradient field in the whitened $\\boldsymbol  \\phi  = \\mathbf  F ^ \\frac  1  2   \\boldsymbol  \\theta  $ parameter space used by NG.  \n",
    "Figure(s) generated by [nat_grad_demo.py](https://github.com/probml/pyprobml/blob/master/scripts/nat_grad_demo.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"nat_grad_demo.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.12:<a name='5.12'></a> <a name='LMS'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Illustration of the LMS algorithm. Left: we start from $\\boldsymbol  \\theta  =(-0.5,2)$ and slowly converging to the least squares solution of $ \\boldsymbol  \\theta   =(1.45, 0.93)$ (red cross). Right: plot of objective function over time. Note that it does not decrease monotonically.  \n",
    "Figure(s) generated by [lms_demo.py](https://github.com/probml/pyprobml/blob/master/scripts/lms_demo.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"lms_demo.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.13:<a name='5.13'></a> <a name='fig:constrOpt'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Illustration of some constrained optimization problems. Red contours are the level sets of the objective function $\\mathcal  L (\\boldsymbol  \\theta  )$. Optimal constrained solution is the black dot, (a) Blue line is the equality constraint $h(\\boldsymbol  \\theta  )=0$. (b) Blue lines denote the inequality constraints $|\\theta _1| + |\\theta _2| \\leq 1$. (Compare to \\cref  fig:L2L1contours  (left).) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"/content/pyprobml/notebooks/figures/images/quadObjectiveLine_theta.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"/content/pyprobml/notebooks/figures/images/{75 100 50 100.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.14:<a name='5.14'></a> <a name='fig:polytope2d'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  (a) A convex polytope in 2d defined by the intersection of linear constraints. (b) Depiction of the feasible set as well as the linear objective function. The red line is a level set of the objective, and the arrow indicates the direction in which it is improving. We see that the optimal solution lies at a vertex of the polytope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"/content/pyprobml/notebooks/figures/images/polytope2d.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.15:<a name='5.15'></a> <a name='fig:projGrad'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Illustration of projected gradient descent. $\\mathbf  w $ is the current parameter estimate, $\\mathbf  w '$ is the update after a gradient step, and $P_ \\mathcal  C  (\\mathbf  w ')$ projects this onto the constraint set $\\mathcal  C $. From   https://bit.ly/3eJ3BhZ  Used with kind permission of Martin Jaggi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"/content/pyprobml/notebooks/figures/images/proj-grad.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.16:<a name='5.16'></a> <a name='emBound'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Illustration of a bound optimization algorithm. Adapted from Figure 9.14 of <a href='#BishopBook'>[Bis06]</a> .  \n",
    "Figure(s) generated by [emLogLikelihoodMax.m](https://github.com/probml/pmtk3/blob/master/demos/emLogLikelihoodMax.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W emLogLikelihoodMax.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.17:<a name='5.17'></a> <a name='MMvsNewton'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  The quadratic lower bound of an MM algorithm (solid) and the quadratic approximation of Newton's method (dashed) superimposed on an empirical density esitmate (dotted). The starting point of both algorithms is the circle. The square denotes the outcome of one MM update. The diamond denotes the outcome of one Newton update. (a) Newton's method overshoots the-se, global maximum. (b) Newton's method results in a reduction of the objective. From Figure 4 of <a href='#Fashing2005'>[MC05]</a> . Used with kind permission of Carlo Tomasi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"/content/pyprobml/notebooks/figures/images/{MMvsNewton}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.18:<a name='5.18'></a> <a name='fig:gmmOldFaithful'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Illustration of the EM for a GMM applied to the Old Faithful data. The degree of redness indicates the degree to which the point belongs to the red cluster, and similarly for blue; thus purple points have a roughly 50/50 split in their responsibilities to the two clusters. Adapted from <a href='#BishopBook'>[Bis06]</a>  Figure 9.8.  \n",
    "Figure(s) generated by [mix_gauss_demo_faithful.py](https://github.com/probml/pyprobml/blob/master/scripts/mix_gauss_demo_faithful.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"mix_gauss_demo_faithful.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.19:<a name='5.19'></a> <a name='gmmSingularity'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  (a) Illustration of how singularities can arise in the likelihood function of GMMs. Here $K=2$, but the first mixture component is a narrow spike (with $\\sigma _1 \\approx 0$) centered on a single data point $x_1$. Adapted from Figure 9.7 of <a href='#BishopBook'>[Bis06]</a> .  \n",
    "Figure(s) generated by [mix_gauss_singularity.py](https://github.com/probml/pyprobml/blob/master/scripts/mix_gauss_singularity.py) [mixGaussMLvsMAP.m](https://github.com/probml/pmtk3/blob/master/demos/mixGaussMLvsMAP.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"mix_gauss_singularity.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W mixGaussMLvsMAP.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.20:<a name='5.20'></a> <a name='gmmLikSurface'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Left: $N=200$ data points sampled from a mixture of 2 Gaussians in 1d, with $\\pi _k=0.5$, $\\sigma _k=5$, $\\mu _1=-10$ and $\\mu _2=10$. Right: Likelihood surface $p( \\mathcal  D  |\\mu _1,\\mu _2)$, with all other parameters set to their true values. We see the two symmetric modes, reflecting the unidentifiability of the parameters.  \n",
    "Figure(s) generated by [mixGaussLikSurfaceDemo.m](https://github.com/probml/pmtk3/blob/master/demos/mixGaussLikSurfaceDemo.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W mixGaussLikSurfaceDemo.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.21:<a name='5.21'></a> <a name='imputeMvnScatter'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Illustration of data imputation. (a) Scatter plot of true values vs imputed values using true parameters. (b) Same as (a), but using parameters estimated with EM. We just show the first four variables, for brevity.  \n",
    "Figure(s) generated by [gaussImputationDemoEM.m](https://github.com/probml/pmtk3/blob/master/demos/gaussImputationDemoEM.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W gaussImputationDemoEM.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.22:<a name='5.22'></a> <a name='fig:rndGrid'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Illustration of grid search (left) vs random search (right). From Figure 1 of <a href='#Bergstra2012'>[JY12]</a> . Used with kind permission of James Bergstra. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"/content/pyprobml/notebooks/figures/images/rndGrid.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    " <a name='Honkela2010'>[Ant+10]</a> H. Antti, R. Tapani, K. Mikael, T. TornioMatti and K. Juha. \"Approximate Riemannian Conjugate Gradient Learning forFixed-Form Variational Bayes\". In: jmlr (2010). \n",
    "\n",
    "<a name='Geron2019'>[Aur19]</a> G. Aur'elien \"Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques for BuildingIntelligent Systems (2nd edition)\". (2019). \n",
    "\n",
    "<a name='BertsimasOpt'>[Ber09]</a> D. Bertsimas \"MIT Class 15.093J: Optimization Methods\". (2009). \n",
    "\n",
    "<a name='BishopBook'>[Bis06]</a> C. Bishop \"Pattern recognition and machine learning\". (2006). \n",
    "\n",
    "<a name='Bergstra2012'>[JY12]</a> B. James and B. Yoshua. \"Random Search for Hyper-Parameter Optimization\". In: jmlr (2012). \n",
    "\n",
    "<a name='Flores-Livas2019'>[Jos+19]</a> F. Jos'eA, B. Lilia, S. Antonio, P. Gianni, A. Ryotaro and E. Mikhail. \"A Perspective on Conventional High-Temperature Superconductorsat High Pressure: Methods and Materials\". In: Phys. Rep. (2019). \n",
    "\n",
    "<a name='Fashing2005'>[MC05]</a> F. Mark and T. Carlo. \"Mean shift is a bound optimization\". In: IEEE Trans. Pattern Anal. Mach. Intell. (2005). \n",
    "\n",
    "<a name='Pascanu2014Thesis'>[Raz14]</a> P. Razvan \"On Recurrent and Deep Neural Networks\". (2014). \n",
    "\n",
    "<a name='ValpolaPhD'>[Val00]</a> H. Valpola \"Bayesian Ensemble Learning for Nonlinear Factor Analysis\". (2000). \n",
    "\n",
    "<a name='Vandenberghe06'>[Van06]</a> L. Vandenberghe \"Applied Numerical Computing: Lecture notes\". (2006). \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
